# -*- coding: utf-8 -*-
"""tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11mLgbCQo-xso0O12TpQp9jgxJGi4RrBr
"""

from google.colab import files
from google.colab import drive
drive.mount('/content/gdrive')
import sys
sys.path.append('/content/gdrive/My Drive/kaggle')
!pip install tqdm --upgrade
!pip install transformers
!pip install tensorflow --upgrade
!pip install tensorflow-gpu

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupKFold
import tensorflow as tf
from transformers import *
import re
!pip install pyspellchecker
from spellchecker import SpellChecker
import string
import tensorflow.keras.backend as K

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

df_train = pd.read_csv('/content/gdrive/My Drive/Colab/tweet/train.csv')
df_test = pd.read_csv('/content/gdrive/My Drive/Colab/tweet/test.csv')
df_sample_submission = pd.read_csv('/content/gdrive/My Drive/Colab/tweet/sample_submission.csv')

def remove_URL(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'',text)

def remove_html(text):
    html=re.compile(r'<.*?>')
    return html.sub(r'',text)

def remove_hashtag(text):
    hashtag=re.compile(r'@{1}\w+')
    return hashtag.sub(r'',text)

def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_punct(text):
    table=str.maketrans('','',string.punctuation)
    return text.translate(table)

spell = SpellChecker()
def correct_spellings(text):
    corrected_text = []
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)

def clean_data(df):
    df['text'] = df['text'].apply(lambda x: x.lower())
    df['text'] = df['text'].apply(lambda x: remove_hashtag(x))
    df['text'] = df['text'].apply(lambda x: remove_URL(x))
    df['text'] = df['text'].apply(lambda x: remove_html(x))
    df['text'] = df['text'].apply(lambda x: remove_emoji(x))
    df['text'] = df['text'].apply(lambda x: correct_spellings(x))
    df['text'] = df['text'].apply(lambda x: remove_punct(x))
    return df

def _convert_to_transformer_inputs(text, tokenizer, max_sequence_length):
    """Converts tokenized input to ids, masks and segments for transformer (including bert)"""
    def return_id(str1, str2, length):
        inputs = tokenizer.encode_plus(str1, str2,
                                       add_special_tokens=True,
                                       max_length=length,
                                       pad_to_max_length = True)

        return [inputs["input_ids"], inputs["token_type_ids"], inputs['attention_mask']]
    
    input_ids, token_type_ids, attention_mask = return_id(text, None, max_sequence_length)
    
    return [input_ids, token_type_ids, attention_mask]

def compute_input_arrays(df, tokenizer, max_sequence_length):
    input_ids, token_type_ids, attention_mask = [], [], []
    df = clean_data(df)

    for t in df['text']:
        ids, tok_ids, atn_mask = _convert_to_transformer_inputs(t, tokenizer, max_sequence_length)

        input_ids.append(ids)
        token_type_ids.append(tok_ids)
        attention_mask.append(atn_mask)
    
    return [np.asarray(input_ids, dtype=np.int32), 
            np.asarray(token_type_ids, dtype=np.int32), 
            np.asarray(attention_mask, dtype=np.int32)]

def compute_output_arrays(df):
    return np.asarray(df['target'])

seq_len =  55
outputs = compute_output_arrays(df_train)
inputs = compute_input_arrays(df_train, tokenizer, seq_len)
test_inputs = compute_input_arrays(df_test, tokenizer, seq_len)
test_outputs = compute_output_arrays(df_test_output)

def create_model():
    in_id = tf.keras.layers.Input((seq_len,), dtype=tf.int32)
    tok = tf.keras.layers.Input((seq_len,), dtype=tf.int32)
    atn = tf.keras.layers.Input((seq_len,), dtype=tf.int32)
    
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')
    
    pooled_output, sequence_output = bert_model([in_id, atn, tok])
    print(pooled_output, sequence_output)
    clf_output = pooled_output[:, 0, :]

    x = tf.keras.layers.Dense(1, activation='sigmoid')(clf_output)
    model = tf.keras.models.Model(inputs=[in_id, atn, tok,], outputs=x)
    
    return model

valid_preds = []
test_preds = []
history = []

train_inputs = inputs
train_outputs = outputs

K.clear_session()
model = create_model()

optimizer = tf.keras.optimizers.SGD(learning_rate=2e-4, momentum=0.8)
model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['acc'])
model.summary()

history = model.fit(train_inputs,
                         train_outputs,
                         epochs=6,
                         batch_size=16,
                         validation_split=0.1)
model.save_weights('bert.h5')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

epochs = range(1, len(history.history['acc']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

y_pre = model.predict(test_inputs)
df_sample_submission['target'] = y_pre.round().astype(int)
df_sample_submission.to_csv('/content/gdrive/My Drive/Colab/tweet/submission.csv', index=False)